{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65c3566-81c6-4d26-a823-738e1140f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets 'accelerate>=0.26.0' -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b94624-bda2-4d81-8b91-abd31569f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"nuprl/engineering-llm-systems\", \"humaneval\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50e6a46-7a58-439f-938d-7de3f7029302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B-Base\") # NON GPU\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68fc97c-13d9-47f7-8c6b-4ce7ce8607e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B-Base\").to(\"mps\") # GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ade3db31-1af2-4fa1-b690-5c61c481cb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shakespeare was a great writer. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great plays. He wrote many great'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inputs = tokenizer(\"Shakespeare was a great\", return_tensors=\"pt\").to(model.device)\n",
    "example_outputs = model.generate(\n",
    "    **example_inputs,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    max_new_tokens=300)\n",
    "tokenizer.decode(example_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9354f44b-67c6-4504-94ab-af79e6004e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_completions(completion, clip_at = [\"\\ndef\", \"\\nclass\", \"\\nif\", \"\\nprint\"]):\n",
    "    # split at each successively\n",
    "    result = completion\n",
    "\n",
    "    for clip in clip_at:\n",
    "        result = result.split(clip)[0]\n",
    "\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88d7cf7-b201-49e8-a632-07b436275ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_completions(prompt, count = 5):\n",
    "    completions = []\n",
    "    for i in range(count):\n",
    "        inputs = tokenizer(prompt, return_tensors = \"pt\").to(model.device)\n",
    "        example_outputs = model.generate(\n",
    "            **inputs,\n",
    "            pad_token_id = tokenizer.eos_token_id,\n",
    "            do_sample = True,\n",
    "            temperature = 0.2,\n",
    "            max_new_tokens = 300)\n",
    "        result = tokenizer.decode(example_outputs[0])\n",
    "\n",
    "        clipped_result = clip_completions(result)\n",
    "        completions.append(clipped_result)\n",
    "        \n",
    "        print(f\"{i + 1}: {result[:100].strip()}...\\n\")\n",
    "\n",
    "        \n",
    "    with open(\"completions.json\", \"w\") as f:\n",
    "        json.dump(completions, f)\n",
    "    \n",
    "    #open and read the file after the appending:\n",
    "    with open(\"completions.json\") as f:\n",
    "        loaded = json.load(f)\n",
    "        if len(loaded) == len(completions):\n",
    "            print(\"successfuly wrote completions\")\n",
    "        else:\n",
    "            print(\"write failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "907c350b-8310-4011-aca1-a6a18c87447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: def bogosort(ary):\n",
      "    while True:\n",
      "        for i in range(len(ary) - 1):\n",
      "            if ary[i] > ary...\n",
      "\n",
      "2: def bogosort(ary):\n",
      "    while True:\n",
      "        for i in range(len(ary)):\n",
      "            if i == 0:...\n",
      "\n",
      "3: def bogosort(ary):\n",
      "    while True:\n",
      "        for i in range(len(ary)):\n",
      "            for j in range(len(...\n",
      "\n",
      "4: def bogosort(ary):\n",
      "    while True:\n",
      "        for i in range(len(ary)):\n",
      "            if i == 0:...\n",
      "\n",
      "5: def bogosort(ary):\n",
      "    while len(ary) > 1:\n",
      "        for i in range(len(ary) - 1):\n",
      "            if ary[...\n",
      "\n",
      "successfuly wrote completions\n"
     ]
    }
   ],
   "source": [
    "generate_completions(\"def bogosort(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "923313a3-9923-4104-895e-a03ce70ebb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e345dac92f114b24b21b2405ec043fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/161 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(data):\n",
    "    inputs = tokenizer(data[\"prompt\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "    return inputs\n",
    "\n",
    "ds_tokenized = ds.map(preprocess, batched=True, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01c465d0-5251-40b1-8d1f-a9ceb3c11647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ld/pgnbqm8506lbybn9m7nkjnk80000gn/T/ipykernel_45096/949983161.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/aidan/Documents/School/LLM-INT/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.607391357421875, 'eval_model_preparation_time': 0.0045, 'eval_runtime': 17.08, 'eval_samples_per_second': 9.426, 'eval_steps_per_second': 1.23}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    per_device_eval_batch_size = 8,\n",
    "    do_eval = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    tokenizer = tokenizer,\n",
    "    eval_dataset = ds_tokenized,\n",
    ")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
