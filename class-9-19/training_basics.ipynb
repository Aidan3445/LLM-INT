{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to show you the basics of training and generation\n",
    "with a single GPU. The Transformers library has a lot of convenient methods\n",
    "for training and generation. We are going to avoid them and instead work directly\n",
    "with the neural network. This is the only way to understand the material.\n",
    "\n",
    "We first load a couple of modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to use PyTorch directly, unless we used the highest-level APIs.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# This is a standard optimizer, which will use during training.\n",
    "from torch.optim import AdamW\n",
    "# Transformers has implementations for essentially every well-known LLM. We\n",
    "# are going to work with what are called causal, auto-regressive, or\n",
    "# decoder-only models. These includes StarCoder, Llama, the GPT models, etc.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Datasets gives convenient access to open source datasets.\n",
    "import datasets\n",
    "# Tqdm makes it easy to get a progress bar during training.\n",
    "from tqdm.auto import tqdm\n",
    "# Matplotlib will help us plot metrics after training.\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads the model and tokenizer. You may need to modify\n",
    "the `MODEL` variable below to load the model from a different path.\n",
    "**If you mess up the model during training, consider re-running the cell below to\n",
    "reload the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL = \"/scratch/bchk/aguha/models/Qwen3-1.7B-Base\"\n",
    "\n",
    "# The model and tokenizer get loaded separately. But, they are typically at the\n",
    "# same location, and it never makes sense to mix-and-match tokenizers and\n",
    "# models.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, padding_side=\"left\")\n",
    "# I don't know why this isn't set by default, but you always want this.\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda:3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer turns an input string into a sequence of tokens, which are the\n",
    "`input_ids` that appear below. We can ignore the `attention_mask` for now.\n",
    "We write `return_tensors=\"pt\"` to get a PyTorch tensor. Without this flag, we\n",
    "get the output as a Python list, which the model cannot use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_inputs = tokenizer([\"def hello():\\n\\treturn\"], return_tensors=\"pt\")\n",
    "example_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth understanding the shape of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a 2D tensor, where the first dimension is the batch and the second\n",
    "dimension is the sequence length. We have a single item in the batch, so the\n",
    "length of the first dimension is 1. The second dimension is 4, so the input was\n",
    "split into 4 tokens.\n",
    "\n",
    "The code below loops through the tokens and *decodes* them back into strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We have a single item in the batch, so just\n",
    "example_input = example_inputs.input_ids[0]\n",
    "# For every token in the sequence\n",
    "for token in example_input:\n",
    "    # Without tok.item() we print tensor(n) instead of n.\n",
    "    # We use __repr__ so that special characeters like newlines are printed.\n",
    "    print(token.item(), \"->\", tokenizer.decode(token).__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do now:** You should try to tokenize a different input string, and run the\n",
    "decoding loop to see how it gets tokenized. I suggest using your name in the\n",
    "function name, e.g., `hello_arjunguha`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will directly use the model to get a sense of what it\n",
    "does. In ordinary usage, we train it in a loop, or we generate output in \n",
    "loop. There will be no loops in this section.\n",
    "\n",
    "We'll use the same example input from the previous section. The model\n",
    "requires the input tensors to be on the same device (the GPU) as the model. The\n",
    "`.to(model.device)` at the end of the next line takes care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_inputs = tokenizer(['def hello():\\n\\treturn'], return_tensors=\"pt\").to(model.device)\n",
    "print(example_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs a *forward pass* with two simplifications:\n",
    "1. We disable dropout (`model.eval`)\n",
    "2. We don't compute gradients (`torch.no_grad`)\n",
    "We'll enable both when we get to training.\n",
    "\n",
    "Notice that the output from the tokenizer is conveniently structured so that we\n",
    "can pass it as keyword arguments to `model.forward` by writing\n",
    "`model.forward(**example_inputs)`. In our case, this is equivalent to:\n",
    "\n",
    "```python\n",
    "model.forward(\n",
    "    input_ids=example_inputs.input_ids,\n",
    "    attention_mask=example_inputs.attention_mask\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    example_outputs = model.forward(**example_inputs)\n",
    "print(example_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above has a lot of optional fields, but the only one that is set is `logits`.\n",
    "Let's compare its shape to the shape of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(example_inputs.input_ids.shape)\n",
    "print(example_outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have one output per input token, and each output is a tensor with ~150,000\n",
    "elements. Let's look at one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_outputs.logits[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each index in this tensor corresponds to a token type ID, and the\n",
    "output represents the distribution over all possible tokens. But, look at the\n",
    "numbers. There are plenty of negative numbers, so this is *not* a probability\n",
    "distribution.\n",
    "\n",
    "These are raw, unnormalized predictions, or scores, or *logits*. We can turn \n",
    "them into a distribution using the *softmax* function. We can also sum them to\n",
    "verify that they sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_dist_single = F.softmax(example_outputs.logits[0, 0], dim=0)\n",
    "print(example_dist_single)\n",
    "print(example_dist_single.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn every output into a distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copied from above\n",
    "example_inputs = tokenizer(['def hello():\\n\\treturn'], return_tensors=\"pt\").to(model.device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    example_outputs = model.forward(**example_inputs)\n",
    "\n",
    "# Notice that we do .logits[0] and not .logits[0,0] as above.\n",
    "example_dist = F.softmax(example_outputs.logits[0], dim=1)\n",
    "print(example_dist.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these distributions (one for each output), we can produce the most likely\n",
    "output token at each position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Input tokens:\", example_inputs.input_ids)\n",
    "for tok in example_inputs.input_ids.cpu().tolist()[0]:\n",
    "    print(tok, \"->\", tokenizer.decode(tok).__repr__())\n",
    "output_tokens = torch.argmax(example_dist, dim=1)\n",
    "print(\"Output tokens:\", output_tokens)\n",
    "for tok in output_tokens.cpu().tolist():\n",
    "    print(tok, \"->\", tokenizer.decode(tok).__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this as follows:\n",
    "\n",
    "1. `find` is the most likely next token after `def`\n",
    "2. `(name` is the most likely next token after `def hello`\n",
    "3. `'    '` (four spaces) is the most likely next token after `def hello():\\n`\n",
    "4. ` \"` is the most likely next token after `def hello(): return `\n",
    "\n",
    "When doing generation, we only care about (4), which is the next token after\n",
    "the full input sequence. But, we happen to get all of this, and it is\n",
    "necessary for training.\n",
    "\n",
    "Now that we've seen the output of a vanilla forward pass, let's use `forward`\n",
    "in a configuration that is closer to what we need to train the model.\n",
    "To train, we have to specify the expected output sequence. The language-modelling\n",
    "objective is to predict the next token in a long text sequence. So, the\n",
    "\"expected output\" is the input itself, but shifted by 1.\n",
    "\n",
    "- Input: `[ \"def\",  \" hello\", \"():\\n\", \"return\" ]`\n",
    "- Expected output: `[ \" hello\", \"():\\n\", \"\\treturn\", ... ]`\n",
    "\n",
    "We don't have a predicted next-token for last input token. So, but that's fine. We can ignore it. Given enough data, we should see that last token again in some other context where it is not the last token!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss\n",
    "\n",
    "When we train a model to \"predict the next token\", we are effectively saying\n",
    "that that any prediction that is not the expected next-token is wrong.\n",
    "This definition has some obvious consequencies:\n",
    "\n",
    "1. If the model assigns probability $p=1$ to the expected token, then the loss\n",
    "   should be as small as possible.\n",
    "\n",
    "2. If the model assigns probability $p=0$ to the expected token, then the loss\n",
    "   should be as large as possible.\n",
    "\n",
    "3. The loss should interpolate smoothly between these two extremes.\n",
    "\n",
    "I encourage you to work through the derivation of cross entropy loss in\n",
    "[Speech and Language Processing] (Chapter 5). But, the upshot is that\n",
    "if the models output distribution is $[p_1, p_2, \\ldots, p_n]$ and $p_k$ is\n",
    "the probability of the expected token, then the loss is:\n",
    "\n",
    "$$- \\log p_k\n",
    "$$\n",
    "\n",
    "When $p_k =1$, this quantity is zero. When $p_k < 1$, the loss will be \n",
    "positive number.\n",
    "\n",
    "*When training with cross-entropy loss, loss cannot be negative.*\n",
    "\n",
    "PyTorch implements cross-entropy loss as `F.cross_entropy`. We give it\n",
    "two arguments:\n",
    "\n",
    "1. A 2D tensor of predictions with shape `(num_items, num_classes)`.\n",
    "2. A 1D tensor of labels with shape `(num_items,)`.\n",
    "\n",
    "The predictions must be *logits*, and not probabilities. i.e., we don't\n",
    "use softmax ourselves. `F.cross_entropy` functions is effectively\n",
    "softmax followed by cross-entropy loss.\n",
    "\n",
    "\n",
    "Consider the following prediction. You can interpret it as follows:\n",
    "\n",
    "1. There are exactly three tokens in the vocabulary.\n",
    "2. The model predicted that token ID 1 is certainly the next token, and token 0 and 2 are effectively impossible.\n",
    "\n",
    "Thus, the loss is as low as possible (zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.tensor([[0.0, 100.0, 0.0 ]])\n",
    "labels = torch.tensor([1])\n",
    "F.cross_entropy(pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we again have three three tokens. The model predicted\n",
    "that the next token is Token ID 0. However, the expected next token is token 1.\n",
    "Thus the loss is very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.tensor([[100.0, 0.0, 00.0 ]])\n",
    "labels = torch.tensor([1])\n",
    "F.cross_entropy(pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below compute the loss for a single item. Notice we write\n",
    "`logits[0,:-1]` to skip the last token predicted, since we don't\n",
    "have a label for the last token. Conversely, we write\n",
    "`example_inputs.input_ids[0, 1:]` to exclude the first token from the\n",
    "list of tokens to predict, since we cannot predict the first token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval() # This is wrong, deliberately. Will fix later.\n",
    "example_inputs = tokenizer(['def hello():\\n\\treturn'], return_tensors=\"pt\").to(model.device)\n",
    "logits = model.forward(**example_inputs).logits\n",
    "F.cross_entropy(logits[0, :-1], example_inputs.input_ids[0, 1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the output above, we now have  `grad_fn`, which allows backpropogation, which is what `loss.backward`\n",
    "  does.\n",
    "\n",
    "In PyTorch, `backward` computes and saves gradients, but *does not \n",
    "update model weights.* You can confirm this by re-running the cell above\n",
    "repeatedly. You'll get exactly the same loss, which indicates the model is\n",
    "not learning anything.\n",
    "\n",
    "To actually update model weights, we need an optimizer. The textbook approach\n",
    "to optimization is *stochastic gradient descent (SGD)*. We are going to use\n",
    "*AdamW*, which is more sophisticated and works better with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the optimizer with two new lines at the end:\n",
    "1. `optimizer.step()` updates weights\n",
    "2. `optimizer.zero_grad()` creates the gradients that `.backward` computes.\n",
    "   *Always* call `.zero_grad` immediately after `.step` for now.\n",
    "\n",
    "The cell below is what we call the *training cell*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "example_inputs = tokenizer(['def hello():\\n\\treturn'], return_tensors=\"pt\").to(model.device)\n",
    "logits = model.forward(**example_inputs).logits\n",
    "loss = F.cross_entropy(logits[0, :-1], example_inputs.input_ids[0, 1:])\n",
    "print(loss)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell above several times. You will probably see loss going down. Model\n",
    "is learning! You can run the cell below to see the predictions. Once the loss\n",
    "above goes down significantly (e.g., below 1.0), you'll see predictions closer\n",
    "to the input sequence.\n",
    "\n",
    "(We are effectively training the model to memorize this string, which is\n",
    "a silly thing to do.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval() # Do NOT change this \n",
    "example_inputs = tokenizer(['def hello():\\n\\treturn'], return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    example_outputs = model.forward(**example_inputs)\n",
    "example_dist = F.softmax(example_outputs.logits[0], dim=1)\n",
    "output_tokens = torch.argmax(example_dist, dim=1)\n",
    "print(\"Output tokens:\", output_tokens)\n",
    "for tok in output_tokens.cpu().tolist():\n",
    "    print(tok, \"->\", tokenizer.decode(tok).__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's address the use of `model.eval()`.  `model.eval()` disables the \n",
    "*dropout* layers. Dropout is essential regularization, but introduces\n",
    "randomness. Enable it by changing `model.eval()` to `model.train()` in the\n",
    "**training cell**. You can re-run training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the code above, you are ready to write a training loop. The basic idea\n",
    "is this:\n",
    "\n",
    "1. Find a dataset to train on\n",
    "2. Loop over the items in the dataset:\n",
    "   - Put the code in the training cell in the loop body.\n",
    "   - Tokenize and train on the each item in the dataset\n",
    "   - Log the loss so that you have some sense of progress.\n",
    "\n",
    "You can can any dataset you like. But, let's work with the [GSM8K] dataset,\n",
    "which is the training set for the *Math Word Problems* homework.\n",
    "\n",
    "[GSM8K]: https://huggingface.co/datasets/nuprl/engineering-llm-systems/viewer/gsm8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = datasets.load_dataset(\"nuprl/engineering-llm-systems\", \"gsm8k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example training item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[5_125])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do now:** Write the training loop below.\n",
    "\n",
    "Some suggestions:\n",
    "\n",
    "1. You should probably work with a prefix of the dataset, e.g., the first\n",
    "   500 items.\n",
    "\n",
    "2. You should also have a way of monitoring progress, e.g., using `tqdm`.\n",
    "\n",
    "3. You need to think about how to format the data. How about formatting it\n",
    "   as a single string, e.g., `\"Question: {question}\\nAnswer: {answer}\"`, which\n",
    "   makes evaluation easy.\n",
    "\n",
    "You can use the following snippet to do both:\n",
    "\n",
    "```\n",
    "for ix in tqdm(range(500)):\n",
    "    # Your code here\n",
    "```\n",
    "\n",
    "3. We recommend creating a list call `losses` to store all the losses\n",
    "   you get. You can then plot the losses you get using `plt.plot(losses)`\n",
    "   at the end of the loop. Here is a plot of losses we got from the first\n",
    "   500 items. Note that there is some randomness, e.g., due to dropout, so\n",
    "   you won't get exactly the same curve. But, a reasonable start looks like this:\n",
    "   a very steep drop followed by much smaller drops.\n",
    "\n",
    "4. You probably should reload the model, especially if you ran the training\n",
    "   cell in *Model Basics* several times. The model loading code is the\n",
    "   Introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "losses = [ ]\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "model.train()\n",
    "for ix in tqdm(range(500)):\n",
    "    item = train_data[ix]\n",
    "    # Format as \"Question: ...\\nAnswer: ...\"\n",
    "    text = f\"Question: {item['question']}\\nAnswer: {item['answer']}\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    pass\n",
    "\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you should evaluate your model. You have code to evaluate on this\n",
    "task from homework. Try to evaluate zero shot and see how you do, both\n",
    "before and after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Left To Do?\n",
    "\n",
    "This is the simplest possible training code. There are a several ways to\n",
    "improve it:\n",
    "\n",
    "To make better use of your GPU:\n",
    "\n",
    "1. You can pack several training items into a single input, separated with the\n",
    "   `<|endoftext|>` token. This packing is quite standard, and you can only\n",
    "   pack up to the maximum supported sequence length. For StarCoder that is\n",
    "   8,192 tokens. But, that requires significant memory. We recommend not\n",
    "   exceeding sequences of length 2,048.\n",
    "2. You can load a batch with more than 1 item. With Flash Attention 2 and\n",
    "   2,048 token sequences, you can probably do batch size 2 or 4 with\n",
    "   StarCoder-1B.\n",
    "\n",
    "To try to learn better:\n",
    "\n",
    "3. You configure the optimizer to use *weight decay*. (Which requires setting\n",
    "   some non-default options to do it right with a Transformer.)\n",
    "4. You can set a learning rate schedule instead of a having a constant\n",
    "   learning rate, or just try different learning rates.\n",
    "\n",
    "To better understand training:\n",
    "\n",
    "5. You can have a validation set, and measure evaluate on the validation set\n",
    "   periodically during training.\n",
    "\n",
    "6. You can save checkpoints for evaluation.\n",
    "\n",
    "The last two items should be done first. We'll get to all of this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "We are *not* going to worry about batching at first. So, you  You can skip this for now, but return here when we get to batching towards the end of the tutorial.\n",
    "\n",
    "When we put 2+ items in a batch, we need to pad the shorter items using `padding=True`.\n",
    "\n",
    "You should modify `inputs` above to include another short string, e.g. \n",
    "`'def fac('` and add `padding=True` as an argument to the tokenizer to enable\n",
    "padding. After that, go ahead and look at `input_ids`. Notice how the\n",
    "`attention_mask` is set. We also recommend rerunning the loop above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
